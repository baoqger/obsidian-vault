
#### 监督微调（SFT）

在本节中，我们通过使用之前准备的合成数据集来微调一个小模型（gpt-4.1-mini），以改善Zava零售代理的工具调用行为。

这次SFT的目标很简单：

***教会模型在现实的多轮对话中可靠地链式调用“find_” → “get_user_details”（以及类似的定义在系统级指令/提示词的模式）。***

#### Zava零售代理的微调目标

基线评估结果已经显示，gpt-4.1-mini在单步工具调用上表现相当不错，但在Agent系统指令要求的流程中却遇到困难，例如：

- 用户认证 → 个人资料查找
- 地址变更流程
- 级联工具链，其中参数必须正确传播

这些失败模式有一个共同的根源：

***基础模型没有学会多步骤工具序列，尤其是当下一个工具依赖于前一个工具的输出时。***

Note: 这里多步骤工具序列, 其实也最多就是6轮

通过针对性的示例进行微调可以帮助模型学习：

- 何时调用find_user_id_by_name_zip
- 何时立即跟随调用get_user_details
- 如何传播返回的ID
- 如何遵循嵌入在agent系统指令中的Zava特定业务规则

这正是SFT在小数据集上带来巨大收益的任务类型，而小模型（如gpt-4.1-mini）受益最大。

#### 准备SFT数据集

对于这次微调，我们从生成的合成数据集开始，仅提取展示我们希望模型学习的行为的对话。

我们使用两个文件：

- data/sft_train.jsonl
- data/sft_test.jsonl

这些文件包含：

- 多轮对话，涉及多个工具调用
- 完整的“messages”数组（系统、用户、助手、工具响应）
- Agent的工具调用，以正确的格式用于SFT
- 加强Agent系统指令行为的模式 (例如，强制的get_user_details链式调用)

⚠️ 注意：我们不会像评估时那样扩展这些数据用于SFT(per tool call的拆分)。SFT受益于完整的对话，包括中间的回合。

### SFT 能解决什么——以及不能解决什么

监督式微调（SFT）能显著改善 Zava 零售代理在最常见的工具调用挑战上的表现，包括：

- 确定性的工具链式调用
- 强制执行系统指令要求的流程顺序
- 正确传递参数
- 消除“幻觉”的工具名称
- 稳定多轮助手行为

然而，SFT 并不能完全解决更复杂的推理模式，例如：

- 较长的工具调用序列（7–10 步）
- 需要中间推理或规划的场景
- 依赖“合成的内部状态”的流程 (flows that depend on synthesized internal state)???
- 对退货政策的细微/精细解释 (nuanced return-policy interpretation)???
- 下一次工具调用依赖多个先前工具结果的情况

这些场景要求模型学会“为什么需要调用某个工具”，而不仅仅是根据训练数据去模仿“下一步会出现哪个工具”。

为应对这些更深层的决策模式，我们在后面引入强化式微调（RFT）：模型学习优化的是整个多步骤序列最终结果的正确性，而不是只模仿下一步操作。