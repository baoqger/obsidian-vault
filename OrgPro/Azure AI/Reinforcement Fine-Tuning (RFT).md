
### 强化式微调（RFT）

监督式微调显著提升了 Zava 零售代理在**短且由系统指令驱动**的工具调用序列上的表现，例如：

- 姓名/邮编查询 → 获取用户档案
- 简单的订单查询
- 修改地址流程

然而，一些客户场景需要跨越多次工具调用进行**更深层的多步骤推理**，包括：

- 审查完整的订单历史
- 检查多个商品的退货资格
- 计算退款路径（礼品卡 vs 原支付方式）
- 核实政策例外情况
- 根据商品类别、时间窗口或状态进行分支逻辑判断(branching logic)

这些复杂工作流通常涉及 **7–10 次**工具调用，并且要求模型能够围绕系统指令进行推理，而不仅仅是模仿“下一步该做什么”。

这正是强化式微调（RFT）变得至关重要的地方。

![[Pasted image 20251224122253.png]]

强化式微调（RFT）训练模型去**最大化**一个基于**整个多轮输出**计算得到的奖励，而不是孤立地预测下一次工具调用。

在我们的场景中，奖励来自一个**自定义的、基于 LLM 的评测器（grader）**，它会检查：

**退货政策正确性**

- 模型是否应用了正确的时间窗口？
- 是否遵守了按品类划分的限制？
- 是否选择了正确的处理方式（退款、换货、门店储值/店铺积分）？

**序列正确性**

- 模型是否以正确的顺序选择了所需工具？
- 是否避免了冗余或无效的工具调用？

**参数正确性**

- 工具调用的输入参数是否有效，并且与先前工具输出保持一致？

**结果正确性**

- 最终总结是否与预期的客户指引一致？

SFT 教会模型如何进行工具链式调用。  
RFT 教会模型一个序列为什么对或错——以及如何优化它。

当工作流存在如下的分支逻辑时，这一点尤其强大，例如：

> “如果商品属于 X 类别且在 15 天内，则允许退货；  
> 否则提供门店储值/店铺积分，或拒绝。”

即使是很强的基础模型，在这类高度依赖决策的场景中也常常表现吃力，除非通过基于奖励的优化，让模型在长对话上进行训练。

在接下来的章节中，我们将准备 RFT 数据集、设计奖励函数，并运行一个 RFT 训练任务，从而显著提升Agent遵循复杂退货政策逻辑的能力(deep return-policy logic)。

