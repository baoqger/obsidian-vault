

https://medium.com/@raktims2210/rlvr-the-training-breakthrough-that-will-make-reasoning-ai-verifiable-cf4209e79669

**RLHF vs RLVR**

大家如果了解过大模型的整个训练过程, 那么对于后训练的RLHF(Reinforcement Learning from Human Feedback), 应该是很熟悉了的.  那么RLVR(Reinforcement Learning from Verifiable Rewards)是什么呢, 它有什么不同?  

**从让模型"友好"到让模型"准确"**

![[Pasted image 20260103200241.png]]

想象一个学生正在为一场考试做准备, 反馈有两种方式：

- **主观反馈**  
    “你的解决方案听起来不错，感觉清晰，写得很好。”
    
- **可验证反馈**  
    “你答对了10道题中的7道. 三道是错的——这是答案和评分标准。”
    

第一种反馈关乎风格和偏好。  
第二种反馈关乎事实和规则。

RLHF就像第一种反馈：

人类比较两个答案，说：“这个更好。”  它在语气、礼貌、安全性和对话质量方面表现出色。

RLVR则是第二种类型：

模型的答案与真实情况、规则或程序验证器进行核对。  只有当答案能够被客观验证为正确时，模型才会获得奖励。

用一句话总结：RLHF训练模型变得讨人喜欢(likable)；RLVR训练模型变得正确, 当然是在“正确”可以被量化验证的领域。

**RLHF的问题

在深度推理和高风险领域时，RLHF有严重的局限。

人类反馈是主观的

- 两位评审可能会对哪个答案“更好”产生分歧。
- 人类有时会奖励那些听起来自信的答案，即使它们是错误的。

无法高效Scaling

- 一方面, 模型训练需要数百万次的训练数据。
- 另一方面, 人工标注昂贵、缓慢，而且常常不一致。

催生表面功夫

- 它会生成看似合理的答案，但无法证明。
- 导致你看到它一本正经的胡说八道。


**RLVR场景1: 数学推导**

![[Pasted image 20260103205157.png]]


每个训练样本数据包括：

- 题目
- 正确的最终答案
- 解题要点或详细步骤

在 RLVR 下：

- 模型读取题目并生成思维链条（chain-of-thought）以及最终答案。
- 验证脚本会检查：
    - 最终答案是否等于正确答案？
    - 中间步骤是否合理正确

- 如果一切匹配，模型会获得高奖励。
- 如果答案错误或推理不一致，则获得低奖励或零奖励。

这样经过多次迭代，模型能学会：
- 探索更长、更谨慎的推理路径
- 自我反思
- 避免那些看似正确的表面功夫

**RLVR场景2: 代码生成**

![[Pasted image 20260103205759.png]]


每个训练样本包含：

- 文档字符串（docstring）或提示词
- 参考行为, 预期应完成的功能
- 一套单元测试用例

在 RLVR 下：

- 模型编写一个函数。
- 验证器：
    - 在沙箱中运行代码
    - 执行单元测试
    - 检查所有测试是否在资源限制内通过
- 如果代码通过所有测试，模型会获得奖励。
- 如果有未通过的测试，模型会获得较低的奖励。

这样, 随着时间推移，模型学到的不仅是“好看”的代码，而是正确、健壮、能通过测试的代码。